{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewkern/disperseNN2/blob/adk_doc/docs/disperseNN2_vignette.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQYmvuPKnf2F"
      },
      "source": [
        "# disperseNN2 Colab Notebook Vignette\n",
        "This notebook is meant to give an example of training a disperseNN2 model on a small dataset.\n",
        "It is meant to be run on Google Colab, which provides a GPU to speed up training but can also be run locally\n",
        "if the user has the required packages installed with or without a GPU. The steps we will take are as follows:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwW_uoEfpMu9"
      },
      "source": [
        "## 1. Set up the environment\n",
        "\n",
        "First we need to set up our colab instance by installing software, installing disperseNN2, cloning the repo to get example data, and importing packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1kf2-NKcTrO"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# install software we will need for the vignette\n",
        "apt-get install poppler-utils pigz -y\n",
        "pip install disperseNN2 pdf2image\n",
        "\n",
        "# clone repo\n",
        "git clone https://github.com/chriscrsmith/disperseNN2.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQONCV7LfWs0"
      },
      "source": [
        "## 2. Grab preprocessed data\n",
        "Rather than wait on simulations, we have created a tarball for this training example that contains preprocessed data.\n",
        "All of the simulations were created following the detailed descriptions [in our documentation](https://dispersenn2.readthedocs.io/en/latest/vignette.html#vignette-simulation). Further the tree sequences were preproccessed using the `disperseNN2 --preprocess` mode and the metadata was\n",
        "extracted according the protocol [here](https://dispersenn2.readthedocs.io/en/latest/vignette.html#vignette-preprocessing)\n",
        " We will download and extract this tarball."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK2Luj82dxvj"
      },
      "outputs": [],
      "source": [
        "# grab data from google drive using gdown\n",
        "!gdown 1eKaX19H0nWneOKi5_tBDpiGMMSaQnMfO\n",
        "# also available by downloading using wget but too slow for colab\n",
        "# wget http://sesame.uoregon.edu/~adkern/vignette.tar.gz .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "uncompress"
      ],
      "metadata": {
        "id": "8SPUgTJMyRQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pigz -d vignette.tar.gz\n",
        "tar xf vignette.tar\n"
      ],
      "metadata": {
        "id": "vLC_RKcbyS-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyrTaHT2nf2K"
      },
      "source": [
        "## 3. Train a model\n",
        "We will train a model on the data we just downloaded. We will use the `disperseNN2 --train` mode to train a model.\n",
        "In the below `disperseNN2` training command, we set pairs to 1000;\n",
        "this is the number of pairs of individuals from each training dataset that are included in the analysis, and we chose 1000 to reduce the memory requirement.  We’ve found that using 100 for `--pairs_encode` works well, and reduces memory significantly. Training takes approximately 20min on a t4 instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywY77fIheFqw"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "disperseNN2 \\\n",
        "             --out vignette/output_dir \\\n",
        "             --seed 12345 \\\n",
        "             --train \\\n",
        "             --max_epochs 100 \\\n",
        "             --validation_split 0.2 \\\n",
        "             --batch_size 10 \\\n",
        "             --learning_rate 1e-4 \\\n",
        "             --pairs 1000 \\\n",
        "             --pairs_encode 100 \\\n",
        "             --gpu any \\\n",
        "             > vignette/output_dir/training_history_12345.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iLkdZ164JzD"
      },
      "source": [
        "Okay training is done! Let's plot the training history and then display it here in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ek6BDz4yJNZ"
      },
      "outputs": [],
      "source": [
        "!disperseNN2 --plot_history vignette/output_dir/training_history_12345.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtXGoVojWRGW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pdf2image import convert_from_path\n",
        "from IPython.display import display, Image\n",
        "images = convert_from_path(\"vignette/output_dir/training_history_12345.txt_plot.pdf\")\n",
        "for i, image in enumerate(images):\n",
        "    fname = \"image\" + str(i) + \".png\"\n",
        "    image.save(fname, \"PNG\")\n",
        "Image(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOMi5kQh2_TU"
      },
      "source": [
        "## 4. Validation\n",
        "Next, we will validate the trained model on simulated test data. In a real application you should hold out datasets from training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEBQ3V-Wnf2N"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "disperseNN2 \\\n",
        "    --out vignette/output_dir \\\n",
        "    --seed 12345 \\\n",
        "\t--predict \\\n",
        "\t--batch_size 10 \\\n",
        "\t--pairs 1000 \\\n",
        "\t--pairs_encode 100 \\\n",
        "\t--num_pred 100 \\\n",
        "\t--gpu any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1F6eOKnf2N"
      },
      "source": [
        "Below is a plot of the predictions, ``vignette/output_dir/Test/predictions_12345.txt``:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "x = pd.read_csv('vignette/output_dir/Test/predictions_12345.txt', sep='\\t', header=None)\n",
        "plt.scatter(x[0], x[1])\n",
        "plt.xlabel('true')\n",
        "plt.ylabel('predicted')"
      ],
      "metadata": {
        "id": "fKE6heo-8UI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiDU7r6hnf2N"
      },
      "source": [
        "looks pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE1OGBKpnf2N"
      },
      "source": [
        "# 5. Empirical application\n",
        "Since we are satisfied with the performance of the model on the held-out test set, we can finally predict σ in our empirical data.\n",
        "\n",
        "Before predicting with ``disperseNN2`` we need both the empirical .vcf and .locs in the same place"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s $PWD/disperseNN2/Examples/VCFs/iraptus.vcf vignette/"
      ],
      "metadata": {
        "id": "1sLfT9L29Pfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv6hHiL_nf2N"
      },
      "source": [
        "And then we can run ``disperseNN2`` to predict σ in the empirical data. We will use the ``--predict`` mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWPcSRYenf2N"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "disperseNN2 \\\n",
        "    --out vignette/output_dir \\\n",
        "    --seed 12345 \\\n",
        "    --predict \\\n",
        "    --empirical vignette/iraptus \\\n",
        "    --batch_size 10 \\\n",
        "    --pairs 1000 \\\n",
        "    --pairs_encode 100 \\\n",
        "    --num_reps 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdczAOvcnf2N"
      },
      "source": [
        "The final empirical results are stored in: ``vignette/output_dir/empirical_12345.txt``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJcD6V4anf2N"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat vignette/output_dir/empirical_12345.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noUsBJW9nf2O"
      },
      "source": [
        "**Interpretation**.\n",
        "The output, $\\sigma$, is an estimate for the standard deviation of the Gaussian dispersal kernel from our training simulations; in addition, the same parameter was used for the mating distance (and competition distance). Therefore, to get the distance to a random parent, i.e., effective $\\sigma$,  we would apply a posthoc correction of $\\sqrt{\\frac{3}{2}} \\times \\sigma$ (see original [disperseNN paper](https://doi.org/10.1093/genetics/iyad068) for details). In this example, we trained with only 100 generations spatial, hence the dispersal rate estimate reflects demography in the recent past.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
